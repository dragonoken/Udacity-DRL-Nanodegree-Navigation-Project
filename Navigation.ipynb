{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "Train an agent to collect tasty yellow bananas! Oh no, not the blue ones!\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"files/images/banana_collector_firstview.gif\" align=\"center\"></td>\n",
    "    <td><img src=\"files/images/banana_collector_topview.gif\" align=\"center\"></td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import Modules & Packages\n",
    "\n",
    "Get all the necessary modules to run this notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T18:59:45.269460Z",
     "start_time": "2019-03-28T18:59:42.161322Z"
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THE LINES IN THIS CELL\n",
    "import os.path as osp\n",
    "from collections import deque\n",
    "from time import sleep\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dqn_agent import Agent\n",
    "\n",
    "STATE_SIZE  = 37\n",
    "ACTION_SIZE = 4\n",
    "BRAIN_NAME  = \"BananaBrain\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Explore the Environment\n",
    "\n",
    "Let's take a look at what this environment looks like.\n",
    "\n",
    "Starting from loading the environment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Finding the Environment File\n",
    "\n",
    "If you haven't already downloaded the environment file, please do so before we begin.\n",
    "\n",
    "The links can be found in my README file [here](https://github.com/dragonoken/Udacity-DRL-Nanodegree-Navigation-Project).\n",
    "\n",
    "Then, the next cell below will automatically find the environment file in the same directory.\n",
    "\n",
    "If you have downloaded the files somewhere else, or renamed it for some reason, you need to specify the file path to the environment file manually at the bottom of the code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T18:59:47.702108Z",
     "start_time": "2019-03-28T18:59:47.678170Z"
    }
   },
   "outputs": [],
   "source": [
    "### Automatic Unity environment search\n",
    "if osp.isfile(\"Banana.app/Contents/MacOS/mac_banana\"):\n",
    "    env_file_path = \"Banana.app/Contents/MacOS/mac_banana\"\n",
    "    print(\"Unity environment for MacOS operating system found!\")\n",
    "elif osp.isfile(\"Banana_Linux/Banana.x86_64\"):\n",
    "    env_file_path = \"Banana_Linux/Banana.x86_64\"\n",
    "    print(\"Unity environment for Linux operating system found! (Using 64bit)\")\n",
    "elif osp.isfile(\"Banana_Linux/Banana.x86\"):\n",
    "    env_file_path = \"Banana_Linux/Banana.x86\"\n",
    "    print(\"Unity environment for Linux operating system found! (Using 32bit)\")\n",
    "elif osp.isfile(\"Banana_Windows_x86_64/Banana.exe\"):\n",
    "    env_file_path = \"Banana_Windows_x86_64/Banana.exe\"\n",
    "    print(\"Unity environment for Windows(64bit) operating system found!\")\n",
    "elif osp.isfile(\"Banana_Windows_x86/Banana.exe\"):\n",
    "    env_file_path = \"Banana_Windows_x86/Banana.exe\"\n",
    "    print(\"Unity environment for Windows(32bit) operating system found!\")\n",
    "else:\n",
    "    print(\"No Unity environment found!\")\n",
    "\n",
    "\n",
    "### Or, uncomment the line below and specify the file path yourself\n",
    "\n",
    "# env_file_path = \"Banana_Windows_x86_64/Banana.exe\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Watch the Environment with Random Actions\n",
    "\n",
    "The next code cell will start the environment and show you how it looks when taking random actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T19:01:10.970650Z",
     "start_time": "2019-03-28T19:00:35.785200Z"
    }
   },
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name=env_file_path)\n",
    "env_info = env.reset(train_mode=False)[BRAIN_NAME] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "t = 0\n",
    "while True:\n",
    "    action = np.random.randint(ACTION_SIZE)        # select an action\n",
    "    env_info = env.step(action)[BRAIN_NAME]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    t += 1\n",
    "    print(\"\\rCurrent score at {}: {}\".format(t, score), end='')\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"\\nFinal Score: {}\".format(score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train Your Own Agent!\n",
    "\n",
    "You can train your own agent to solve this environment in this section!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Set Hyperparameters and Training Parameters\n",
    "\n",
    "You will be using my baseline agent algorithm, but you can totally adjust many of its hyperparameters in the cell below!\n",
    "\n",
    "Try playing with various hyperparameters and training parameters here.\n",
    "\n",
    "See if you can find a set of parameters that can solve the environment with the least number of episodes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T19:01:26.747398Z",
     "start_time": "2019-03-28T19:01:26.732437Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "hyperparams = {\n",
    "    'seed':                0,       # random seed\n",
    "    'buffer_size':         100000,  # size of the experience replay buffer\n",
    "    'batch_size':          8,       # number of experiences to sample at each learning step\n",
    "    'start_since':         8,       # number of experiences to store before it begins learning (must be bigger than 'batch_size')\n",
    "    'gamma':               0.99,    # discount factor\n",
    "    'target_update_every': 1,       # how often to update the target network\n",
    "    'tau':                 1e-3,    # how much to update the target network at every update\n",
    "    'lr':                  5e-4,    # learning rate\n",
    "    'update_every':        1,       # how often to update the online network\n",
    "    'priority_eps':        1e-3,    # small values added to priorities in order to have nonzero priorities\n",
    "    'a':                   0.5,     # priority exponent parameter\n",
    "    'n_multisteps':        3,       # number of steps to consider for multistep learning\n",
    "    'v_min':               -10,     # minimum support value for distributional learning\n",
    "    'v_max':               10,      # maximum support value for distributional learning\n",
    "    'n_atoms':             51,      # number of supports for distributional learning\n",
    "    'initial_sigma':       0.05,    # initial noise parameter value for noisy net\n",
    "    'linear_type':         'noisy', # which linear layers to use ('linear' or 'noisy')\n",
    "    'factorized':          False,   # whether to use factorized gaussian noise or not\n",
    "    'clip':                None     # gradient clipping\n",
    "}\n",
    "\n",
    "# Training Parameters\n",
    "\n",
    "train_params = {\n",
    "    'n_episodes': 250,                                    # number of episodes to train the agent for\n",
    "    'eps_start':  0.,   'eps_end':  0.,  'eps_decay': 0., # initial, minimum epsilon values and decay rate for epsilon-greedy policy\n",
    "    'beta_start': 0.4,   'beta_end': 1.0                  # importance-sampling weight for prioritized experience replay\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Initialize the Agent\n",
    "\n",
    "The next code cell below will now initialize an agent using the hyperparameter settings above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T19:01:28.520195Z",
     "start_time": "2019-03-28T19:01:28.484282Z"
    }
   },
   "outputs": [],
   "source": [
    "agent = Agent(state_size=STATE_SIZE, action_size=ACTION_SIZE, **hyperparams)\n",
    "\n",
    "agent.qnetwork_local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Train the Agent\n",
    "\n",
    "Finally, you can start training your agent in the cell below!\n",
    "\n",
    "This process will take some time, depending on your computer's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T19:02:42.764120Z",
     "start_time": "2019-03-28T19:01:32.079979Z"
    },
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name=env_file_path, seed=hyperparams['seed'], no_graphics=False)\n",
    "\n",
    "def dqn(n_episodes=2000,\n",
    "        eps_start=1.0, eps_end=0.01, eps_decay=0.995,\n",
    "        beta_start=0., beta_end=1.0,\n",
    "        continue_after_solved=True):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "        beta_start (float): initial importance-sampling weight for prioritized experience replay\n",
    "        beta_end (float): final importance-sampling weight for prioritized experience replay\n",
    "        continue_after_solved (bool): whether to continue training after reaching the average score 12\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "\n",
    "    agent.beta = beta_start\n",
    "    beta_increment = (beta_end - beta_start) / n_episodes\n",
    "    \n",
    "    noisy_params = [param for name, param in agent.qnetwork_local.named_parameters() if name.endswith('noisy_weight') or name.endswith('noisy_bias')]\n",
    "        \n",
    "    solved = False\n",
    "    epi_str_max_len = len(str(n_episodes))\n",
    "    total_steps_str_max_len = len(str(n_episodes * 300))\n",
    "    n_steps_taken = 0\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[BRAIN_NAME]\n",
    "        state = env_info.vector_observations[0]\n",
    "        pos_score = 0\n",
    "        neg_score = 0\n",
    "        t = 0\n",
    "        while True:\n",
    "            action     = int(agent.act(state, eps))\n",
    "            env_info   = env.step(action)[BRAIN_NAME]\n",
    "            next_state = env_info.vector_observations[0]\n",
    "            reward     = env_info.rewards[0]\n",
    "            done       = env_info.local_done[0]\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            if reward > 0:\n",
    "                pos_score += reward\n",
    "            elif reward < 0:\n",
    "                neg_score += reward\n",
    "            t += 1\n",
    "            if done:\n",
    "                break\n",
    "        else: # if not done (reached max_t)\n",
    "            agent.memory.reset_multisteps()\n",
    "        n_steps_taken += t\n",
    "        score = pos_score + neg_score\n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        agent.beta = min(beta_end, agent.beta + beta_increment)\n",
    "        report_str = \"\\rEpisode {:>{epi_max_len}d} | Total Steps: {:>{total_steps_max_len}d} | Current Score: {:>3.0f}\"\\\n",
    "                     \" | Positive Score: {:>3.0f} | Negative Score: {:>3.0f} | Average Score: {:>6.2f} | Epsilon: {:>6.4f}\"\\\n",
    "                     \" | A: {:>6.4f} | Beta: {:>6.4f}\"\\\n",
    "                     .format(i_episode, n_steps_taken, score, pos_score, neg_score, np.mean(scores_window),\n",
    "                             eps, agent.a, agent.beta, epi_max_len=epi_str_max_len,\n",
    "                             total_steps_max_len=total_steps_str_max_len)\n",
    "        if noisy_params:\n",
    "            flattened_abs_noise = np.concatenate([param.data.abs().cpu().numpy().reshape((-1,)) for param in noisy_params])\n",
    "            report_str += \" | Avg Noise Magnitude: {:>6.4f} +- {:<6.4f}\".format(np.mean(flattened_abs_noise), np.std(flattened_abs_noise))\n",
    "        report_str += \"          \"\n",
    "        if i_episode % 100 == 0:\n",
    "            report_str += '\\n'\n",
    "        print(report_str, end=\"\")\n",
    "        if not solved and np.mean(scores_window)>=12.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            solved = True\n",
    "            if not continue_after_solved:\n",
    "                break\n",
    "    return scores\n",
    "\n",
    "scores = dqn(**train_params,\n",
    "             continue_after_solved=True)\n",
    "\n",
    "env.close()\n",
    "\n",
    "# plot the scores\n",
    "plt.rcParams['figure.facecolor'] = 'w'\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Save the Trained Agent\n",
    "\n",
    "Don't forget to save your trained agent!\n",
    "\n",
    "Save your agents parameters, hyperparameters, and training parameters along with the scores it received during the training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T19:06:00.377783Z",
     "start_time": "2019-03-28T19:06:00.367810Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save({'model_dict': agent.qnetwork_local.state_dict(),\n",
    "            'scores': scores,\n",
    "            'hyperparams': hyperparams,\n",
    "            'train_params': train_params}, 'my_agent.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Watch the Smart Agent\n",
    "\n",
    "Now it's time to watch the smart agent—my pretrained one or your own—collecting delicious bananas!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Loading the Saved Agent Model\n",
    "\n",
    "You first need to load the saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T19:06:17.774651Z",
     "start_time": "2019-03-28T19:06:17.767669Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_model(filepath):\n",
    "    loaded_dict = torch.load(filepath, map_location=torch.device('cpu'))\n",
    "    agent = Agent(state_size=STATE_SIZE, action_size=ACTION_SIZE, **loaded_dict['hyperparams'])\n",
    "    agent.qnetwork_local.load_state_dict(loaded_dict['model_dict'])\n",
    "    agent.qnetwork_target.load_state_dict(loaded_dict['model_dict'])\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit the file path in the cell below according to the location of the save file you want to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T19:06:18.787185Z",
     "start_time": "2019-03-28T19:06:18.744300Z"
    }
   },
   "outputs": [],
   "source": [
    "agent = load_model(\"./pretrained.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Watch the Smart Agent\n",
    "\n",
    "Now, the next cell will show you the loaded agent in action in real time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T19:07:00.900724Z",
     "start_time": "2019-03-28T19:06:20.771444Z"
    }
   },
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name=env_file_path, no_graphics=False)\n",
    "\n",
    "env_info = env.reset(train_mode=False)[BRAIN_NAME] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "t = 0                                              # initialize the time step count\n",
    "while True:\n",
    "    action = int(agent.act(state))                 # select an action\n",
    "    env_info = env.step(action)[BRAIN_NAME]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    t += 1\n",
    "    print(\"\\rCurrent score at {}: {}\".format(t, score), end='')\n",
    "    sleep(0.025)\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "\n",
    "print(\"\\nScore: {}\".format(score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Performance Estimation\n",
    "\n",
    "Usually it's better to run many episodes to get a much accurate estimate of an agent's performance.\n",
    "\n",
    "The code cell below will run the agent at a fast-forward speed and show you the average performance over multiple episodes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T19:10:02.250672Z",
     "start_time": "2019-03-28T19:07:13.274108Z"
    }
   },
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name=env_file_path, no_graphics=False)\n",
    "\n",
    "agent.qnetwork_local.noise(False)\n",
    "\n",
    "n_episodes = 100    # total number of episodes to run\n",
    "total_score = 0     # initialize the cumulative score\n",
    "last_score = 0      # initualize the previous score\n",
    "report_str_format = \"\\rEpisode {{{}}} | Current Score {{:>4.1f}} | Last Score {{:>4.1f}} | Average Score {{:>5.2f}}\".format(\":>{}d\".format(len(str(n_episodes))))\n",
    "for n in range(1, n_episodes + 1):\n",
    "    env_info = env.reset(train_mode=True)[BRAIN_NAME]  # reset the environment\n",
    "    state = env_info.vector_observations[0]            # get the current state\n",
    "    score = 0                                          # initialize the score\n",
    "    while True:\n",
    "        action = int(agent.act(state))                 # select an action\n",
    "        env_info = env.step(action)[BRAIN_NAME]        # send the action to the environment\n",
    "        next_state = env_info.vector_observations[0]   # get the next state\n",
    "        reward = env_info.rewards[0]                   # get the reward\n",
    "        done = env_info.local_done[0]                  # see if episode has finished\n",
    "        score += reward                                # update the score\n",
    "        state = next_state                             # roll over the state to next time step\n",
    "        print(report_str_format.format(n, score, last_score, total_score / n), end='')\n",
    "        if done:                                       # exit loop if episode finished\n",
    "            break\n",
    "    last_score = score\n",
    "    total_score += score\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank you!\n",
    "\n",
    "I hope you had fun here!\n",
    "\n",
    "Happy Deep Reinforcement Learning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notify_time": "5"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
